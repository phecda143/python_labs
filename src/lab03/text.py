import re


def normalize(text: str, *, casefold: bool = True, yo2e: bool = True):
    """
    –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞:
        –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É(–µ—Å–ª–∏ casefold=True)
        –∑–∞–º–µ–Ω—è–µ—Ç —ë/–Å –Ω–∞ –µ/–ï(–µ—Å–ª–∏ yo2e=True)
        —É–±–∏—Ä–∞–µ—Ç –Ω–µ–≤–∏–¥–∏–º—ã–µ —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã
        —Å—Ö–ª–æ–ø—ã–≤–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã –≤ –æ–¥–∏–Ω
    """
    if isinstance(text, str) and isinstance(casefold, bool) and isinstance(yo2e, bool):
        if casefold:
            text = text.casefold()
        if yo2e:
            text = text.replace("—ë", "–µ")
            text = text.replace("–Å", "–ï")
        if "\t" in text or "\n" in text or "\r" in text:
            text = text.replace("\t", " ").replace("\n", " ").replace("\r", " ")
    return " ".join(text.split())


def tokenize(text: str) -> list[str]:
    """
    —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:
        —Å–ª–æ–≤–æ–º —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤ \w (–±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ)
        –¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è –¥–µ—Ñ–∏—Å –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞
        —á–∏—Å–ª–∞ —Ç–∞–∫–∂–µ —è–≤–ª—è—é—Ç—Å—è —Å–ª–æ–≤–∞–º–∏
        —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –≤—Å–µ –Ω–µ–±—É–∫–≤–µ–Ω–Ω–æ-—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã
    """
    if isinstance(text, str):
        text = re.sub(r"[^\w-]", " ", text).split()
        """ re.sub - —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–º–µ–Ω—ã –≤ —Ä–µ–≥—É–ª—è—Ä–∫–∞—Ö
            r'[^\w-] - –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∫—Ä–æ–º–µ \w (–±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã/–ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ) –∏ –¥–µ—Ñ–∏—Å–∞
        """
    return text


def count_freq(tokens: list[str]) -> dict[str, int]:
    """–ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å"""
    if isinstance(tokens, list) and all(isinstance(item, str) for item in tokens):
        character_counting = dict()
        for i in sorted(set(tokens)):
            character_counting[i] = tokens.count(i)
    return character_counting


def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø n –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞–º—Å—Ç–æ—Ç—ã; –ø—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ - –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É —Å–ª–æ–≤–∞"""
    if isinstance(freq, dict) and all(
        isinstance(key, str) and isinstance(value, int) for key, value in freq.items()
    ):
        sorted_items = sorted(freq.items(), key=lambda x: (-x[1], x[0]))
        """.items - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ
            key=lambda x —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á—É lambda x, –≥–¥–µ x –∞–Ω–æ–Ω–∏–º–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            -x[1] –±–µ—Ä–µ—Ç –≤—Ç–æ—Ä–æ–π —ç–ª–µ–º–µ–Ω—Ç(—á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤–∞) –ø–æ —É–±—ã–≤–∞–Ω–∏—é
            x[0] –±–µ—Ä–µ—Ç –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç(—Å–∞–º–æ —Å–ª–æ–≤–æ)
        """
    return sorted_items[:n]


print("normalize")
print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"))
print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞"))
print(normalize("Hello\r\nWorld"))
print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))
print()
print("tokenize")
print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
print(tokenize("hello,world!!!"))
print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))
print(tokenize("2025 –≥–æ–¥"))
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))
print()
print("count_freq")
print(count_freq(["a", "b", "a", "c", "b", "a"]))
print(count_freq(["bb", "aa", "bb", "aa", "cc"]))
print()
print("top_n")
print(top_n({"a": 3, "b": 2, "c": 1}, n=2))
print(top_n({"aa": 2, "bb": 2, "cc": 1}, n=2))
